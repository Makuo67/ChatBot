{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Makuo67/QA_Chat_Bot/blob/master/MoneyBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaOnDw_gZUX5",
        "outputId": "11da2645-82d2-45b3-ae86-5711fb577b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#importing nltk and necessary downloads\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Im using porter stemmer here\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "nltk.download('all')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7tkCksUD0Sh",
        "outputId": "a156da2e-f7ab-4a23-dac4-aca31416cc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             intents\n",
            "0  {'tag': 'welcome', 'patterns': ['Hi', 'Hello',...\n",
            "1  {'tag': 'endingnote', 'patterns': ['see you. b...\n",
            "2  {'tag': 'name', 'patterns': ['whats you name?'...\n",
            "3  {'tag': 'shopping', 'patterns': ['I would like...\n",
            "4  {'tag': 'time period', 'patterns': ['till what...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "intents = pd.read_json(\"https://raw.githubusercontent.com/gopikasr/Chat-Bot---Financial-digital-Assistant/main/DataChatBot.json\")\n",
        "print(intents.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLxiP-WpD0Zg",
        "outputId": "d2b06ded-c1f4-4ef1-86a7-728b7a2f8722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.3.0+cu121)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchviz)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchviz)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchviz)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchviz)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchviz)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchviz)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchviz)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchviz)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4132 sha256=8c5471fae25114af77661b0ef7263a5b75f08f8124025ff91e43437e09ff2163\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchviz\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchviz-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDv1ZyNFD0i4"
      },
      "outputs": [],
      "source": [
        "from torchviz import make_dot,make_dot_from_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NGpVTnZ7ZZQ"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "    return word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoXtuJEs7Zkn"
      },
      "outputs": [],
      "source": [
        "#using porterstemmer here\n",
        "stemmer = PorterStemmer()\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T45n7da94Zk"
      },
      "outputs": [],
      "source": [
        "def BagOfWords(tokenized_sentence, words):\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    Bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words:\n",
        "            Bag[idx] = 1\n",
        "    return Bag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt18wuoMHh97",
        "outputId": "aa9dea30-5df0-47be-a136-09947c31ed88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#checking whether this is working or not\n",
        "sentence=[\"hi\",\"im\",\"gopika\",\"hello\"]\n",
        "words=[\"hi\",\"how\",\"are\",\"you\"]\n",
        "BagOfWords(sentence,words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWjYa9A5HlbZ",
        "outputId": "1790e262-cbae-48db-ba10-206892275628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "298 patterns\n",
            "47 tags: ['Asset allocation', 'AsymmetricInformation', 'Balance sheet', 'Bank Run', 'Bank capital', 'Bank reserves', 'Bargain offer', 'Bond market', 'Capital market', 'Capital ratio', 'Compound interest', 'Credit risk', 'Crowdfunding', 'Demonetisation', 'FICO score', 'Financial crisis', 'Monetary stability', 'Moral hazard', 'Net Worth', 'PRA', 'Ring Fence', 'Term life insurance', 'Thank you', 'VAR', 'bail-out', 'bank', 'bank assests', 'bitcoin', 'cryptocurrency', 'deliveryoption', 'endingnote', 'leverage', 'liquidity', 'liquidity trap', 'location', 'name', 'narrow money', 'nominal interest rate', 'passporting', 'payments', 'price of a saree', 'price of formal wear', 'real interest rate', 'shopping', 'time period', 'todaysOffers', 'welcome']\n",
            "209 unique stemmed words: [\"'s\", '2016', '`', 'a', 'about', 'accept', 'address', 'again', 'all', 'alloc', 'an', 'and', 'ani', 'are', 'asset', 'asymmetr', 'at', 'author', 'averag', 'bail-out', 'bailout', 'balanc', 'bank', 'bargain', 'be', 'bitcoin', 'black', 'bond', 'brand', 'buy', 'by', 'bye', 'call', 'can', 'capit', 'card', 'cash', 'chatbot', 'cheap', 'cheaper', 'close', 'cloth', 'collect', 'compound', 'cost', 'countri', 'credit', 'crisi', 'critic', 'crowdfund', 'cryptocurr', 'day', 'deliv', 'deliveri', 'demonet', 'demonetis', 'do', 'doe', 'enjoy', 'enough', 'everyth', 'exactli', 'exampl', 'expens', 'experi', 'fenc', 'fico', 'financ', 'financi', 'for', 'formal', 'from', 'fun', 'glad', 'go', 'good', 'goodby', 'got', 'happen', 'happi', 'have', 'hazard', 'hello', 'help', 'here', 'hey', 'hi', 'home', 'how', 'i', 'im', 'in', 'india', 'inform', 'insur', 'interest', 'interst', 'is', 'it', 'know', 'later', 'leav', 'leverag', 'life', 'like', 'liquid', 'locat', 'm', 'market', 'mastercard', 'me', 'mean', 'meet', 'monetari', 'money', 'monitari', 'moral', 'more', 'much', 'name', 'narrow', 'net', 'networth', 'new', 'nice', 'nomin', 'occur', 'of', 'offer', 'ok', 'onli', 'open', 'option', 'or', 'our', 'pass', 'passport', 'pay', 'peopl', 'port', 'pra', 'price', 'product', 'provid', 'prudenti', 'rate', 'ratio', 'real', 'recommend', 'regul', 'reserv', 'restaur', 'ring', 'ring-fenc', 'risk', 'robot', 'run', 's', 'sare', 'say', 'score', 'see', 'session', 'sheet', 'shoe', 'shop', 'site', 'situat', 'so', 'someth', 'stabil', 'store', 'suit', 'sunday', 'support', 'take', 'tell', 'term', 'thank', 'that', 'the', 'then', 'there', 'thi', 'thing', 'till', 'time', 'to', 'today', 'trap', 'u', 'up', 'valu', 'var', 'wa', 'want', 'wear', 'what', 'when', 'where', 'which', 'who', 'will', 'work', 'worth', 'would', 'you', 'your', '’']\n"
          ]
        }
      ],
      "source": [
        "#for collecting all the words, lets create an empty list\n",
        "all_the_words = []\n",
        "#for collecting all the tags\n",
        "tags = []\n",
        "#the below pair list which will be  made filled with both patterns and their tags\n",
        "pair = []\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    # adding this to tag list\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenizing each word in the sentence\n",
        "        w = tokenize(pattern)\n",
        "        # addding to our words list(since w is an array we need to use extend for adding the elements)\n",
        "        all_the_words.extend(w)\n",
        "        # add to pair\n",
        "        pair.append((w, tag))\n",
        "\n",
        "# stem and lower each word. So, first excluding punctuation marks\n",
        "ignore_words = ['?', '.', '!',',']\n",
        "all_the_words = [stem(w) for w in all_the_words if w not in ignore_words]\n",
        "# remove duplicates and sort\n",
        "all_the_words = sorted(set(all_the_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(pair), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_the_words), \"unique stemmed words:\", all_the_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9r-KFxY7Zq4",
        "outputId": "8788e69f-91d8-4bc5-b1bc-f2231c6931b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputsize= 209\n",
            "outputsize= 47\n"
          ]
        }
      ],
      "source": [
        "# create training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "#using a tuple to run through the pair\n",
        "for (pattern_sentence, tag) in pair:\n",
        "    # X-->is the bag of words for each pattern_sentence\n",
        "    bag = BagOfWords(pattern_sentence, all_the_words)\n",
        "    X_train.append(bag)\n",
        "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot.so defining label\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "#array\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Hyper-parameters are--->\n",
        "num_epochs = 1000\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "#len(X_train[0]) means the length of 1st bag of words because they all have the same size.\n",
        "#if we want we can just print and check. But its clear here.\n",
        "input_size = len(X_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "#lets print the values\n",
        "print(\"inputsize=\",input_size)\n",
        "print(\"outputsize=\",output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFif38pv9urO"
      },
      "outputs": [],
      "source": [
        "#traning dataset\n",
        "class ChatDataset(Dataset):\n",
        "#implementing init function\n",
        "    def __init__(self):\n",
        "      #storing the values\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSUJO5GX9uwX"
      },
      "outputs": [],
      "source": [
        "#creating a new class for our neural network\n",
        "class NeuralNetModel(nn.Module):\n",
        "  #This will be a feed forward neural network with two hidden layer\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNetModel, self).__init__()\n",
        "        #creating the firstlinear layer. This gets the input size and then the connected hiddenlayer\n",
        "        self.linearlayer1 = nn.Linear(input_size, hidden_size)\n",
        "        #applying batch normalization\n",
        "        self.bn1= nn.BatchNorm1d(hidden_size)\n",
        "        #creating the 1st hidden layer with input size as hiddensize and output size as the hidden size\n",
        "        self.linearlayer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn2= nn.BatchNorm1d(hidden_size)\n",
        "        #creating the 2nd hidden layer with input size as hiddensize and output size as the num classes\n",
        "        self.linearlayer3 = nn.Linear(hidden_size, num_classes)\n",
        "        #using relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "    #implementing the forward pass\n",
        "    def forward(self, x):\n",
        "      #apply our first linear layer which gets x as input and then gives out the output\n",
        "        output = self.linearlayer1(x)\n",
        "        #activation function\n",
        "        output = self.relu(output)\n",
        "      #apply our first linear layer which gets output as input and then gives out the next output\n",
        "        output = self.linearlayer2(output)\n",
        "        #activation function\n",
        "        output = self.relu(output)\n",
        "        output = self.linearlayer3(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7RQ17Gk9u2M"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Create input tensor and move it to the same device as the model\n",
        "x = torch.randn(47, 209).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "cJWZvpM9BK-V",
        "outputId": "6dde3231-4242-4a83-bfa3-6634d24523c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"487pt\" height=\"600pt\"\n viewBox=\"0.00 0.00 487.00 600.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 596)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-596 483,-596 483,4 -4,4\"/>\n<!-- 137605352763488 -->\n<g id=\"node1\" class=\"node\">\n<title>137605352763488</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"241.5,-31 187.5,-31 187.5,0 241.5,0 241.5,-31\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 137605363610000 -->\n<g id=\"node2\" class=\"node\">\n<title>137605363610000</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-86 167,-86 167,-67 262,-67 262,-86\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 137605363610000&#45;&gt;137605352763488 -->\n<g id=\"edge21\" class=\"edge\">\n<title>137605363610000&#45;&gt;137605352763488</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-66.79C214.5,-60.07 214.5,-50.4 214.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-41.19 214.5,-31.19 211,-41.19 218,-41.19\"/>\n</g>\n<!-- 137605363608656 -->\n<g id=\"node3\" class=\"node\">\n<title>137605363608656</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"265,-141 164,-141 164,-122 265,-122 265,-141\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 137605363608656&#45;&gt;137605363610000 -->\n<g id=\"edge1\" class=\"edge\">\n<title>137605363608656&#45;&gt;137605363610000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-121.75C214.5,-114.8 214.5,-104.85 214.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-96.09 214.5,-86.09 211,-96.09 218,-96.09\"/>\n</g>\n<!-- 137605363611344 -->\n<g id=\"node4\" class=\"node\">\n<title>137605363611344</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"143,-196 42,-196 42,-177 143,-177 143,-196\"/>\n<text text-anchor=\"middle\" x=\"92.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363611344&#45;&gt;137605363608656 -->\n<g id=\"edge2\" class=\"edge\">\n<title>137605363611344&#45;&gt;137605363608656</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.1,-176.98C131.82,-168.42 162.45,-155.11 185.08,-145.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.76,-148.37 194.54,-141.17 183.98,-141.94 186.76,-148.37\"/>\n</g>\n<!-- 137605352963616 -->\n<g id=\"node5\" class=\"node\">\n<title>137605352963616</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"146,-262 27,-262 27,-232 146,-232 146,-262\"/>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.bias</text>\n<text text-anchor=\"middle\" x=\"86.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (47)</text>\n</g>\n<!-- 137605352963616&#45;&gt;137605363611344 -->\n<g id=\"edge3\" class=\"edge\">\n<title>137605352963616&#45;&gt;137605363611344</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.95,-231.84C88.73,-224.21 89.71,-214.7 90.56,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"94.06,-206.57 91.6,-196.27 87.1,-205.86 94.06,-206.57\"/>\n</g>\n<!-- 137605363612784 -->\n<g id=\"node6\" class=\"node\">\n<title>137605363612784</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"262,-196 167,-196 167,-177 262,-177 262,-196\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 137605363612784&#45;&gt;137605363608656 -->\n<g id=\"edge4\" class=\"edge\">\n<title>137605363612784&#45;&gt;137605363608656</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-176.75C214.5,-169.8 214.5,-159.85 214.5,-151.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-151.09 214.5,-141.09 211,-151.09 218,-151.09\"/>\n</g>\n<!-- 137605363611200 -->\n<g id=\"node7\" class=\"node\">\n<title>137605363611200</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"265,-256.5 164,-256.5 164,-237.5 265,-237.5 265,-256.5\"/>\n<text text-anchor=\"middle\" x=\"214.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 137605363611200&#45;&gt;137605363612784 -->\n<g id=\"edge5\" class=\"edge\">\n<title>137605363611200&#45;&gt;137605363612784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M214.5,-237.37C214.5,-229.25 214.5,-216.81 214.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"218,-206.17 214.5,-196.17 211,-206.17 218,-206.17\"/>\n</g>\n<!-- 137605363610576 -->\n<g id=\"node8\" class=\"node\">\n<title>137605363610576</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"116,-322.5 15,-322.5 15,-303.5 116,-303.5 116,-322.5\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363610576&#45;&gt;137605363611200 -->\n<g id=\"edge6\" class=\"edge\">\n<title>137605363610576&#45;&gt;137605363611200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M85.59,-303.37C111.14,-292.39 155.46,-273.36 184.89,-260.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.44,-263.86 194.25,-256.7 183.68,-257.43 186.44,-263.86\"/>\n</g>\n<!-- 137605356740160 -->\n<g id=\"node9\" class=\"node\">\n<title>137605356740160</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"119,-394 0,-394 0,-364 119,-364 119,-394\"/>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.bias</text>\n<text text-anchor=\"middle\" x=\"59.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n</g>\n<!-- 137605356740160&#45;&gt;137605363610576 -->\n<g id=\"edge7\" class=\"edge\">\n<title>137605356740160&#45;&gt;137605363610576</title>\n<path fill=\"none\" stroke=\"black\" d=\"M60.83,-363.8C61.68,-354.7 62.8,-342.79 63.73,-332.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"67.22,-333.13 64.67,-322.84 60.25,-332.47 67.22,-333.13\"/>\n</g>\n<!-- 137605363610384 -->\n<g id=\"node10\" class=\"node\">\n<title>137605363610384</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"235,-322.5 140,-322.5 140,-303.5 235,-303.5 235,-322.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n</g>\n<!-- 137605363610384&#45;&gt;137605363611200 -->\n<g id=\"edge8\" class=\"edge\">\n<title>137605363610384&#45;&gt;137605363611200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.14,-303.37C195.14,-293.88 201.69,-278.36 206.84,-266.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.08,-267.48 210.74,-256.91 203.63,-264.76 210.08,-267.48\"/>\n</g>\n<!-- 137605363610816 -->\n<g id=\"node11\" class=\"node\">\n<title>137605363610816</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"238,-388.5 137,-388.5 137,-369.5 238,-369.5 238,-388.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 137605363610816&#45;&gt;137605363610384 -->\n<g id=\"edge9\" class=\"edge\">\n<title>137605363610816&#45;&gt;137605363610384</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-369.37C187.5,-360.16 187.5,-345.29 187.5,-333.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-332.91 187.5,-322.91 184,-332.91 191,-332.91\"/>\n</g>\n<!-- 137605363613120 -->\n<g id=\"node12\" class=\"node\">\n<title>137605363613120</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"121,-454.5 20,-454.5 20,-435.5 121,-435.5 121,-454.5\"/>\n<text text-anchor=\"middle\" x=\"70.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363613120&#45;&gt;137605363610816 -->\n<g id=\"edge10\" class=\"edge\">\n<title>137605363613120&#45;&gt;137605363610816</title>\n<path fill=\"none\" stroke=\"black\" d=\"M86.28,-435.37C105.91,-424.63 139.64,-406.18 162.73,-393.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"164.5,-396.57 171.6,-388.7 161.14,-390.43 164.5,-396.57\"/>\n</g>\n<!-- 137605356745520 -->\n<g id=\"node13\" class=\"node\">\n<title>137605356745520</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"125,-526 6,-526 6,-496 125,-496 125,-526\"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.bias</text>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-503\" font-family=\"monospace\" font-size=\"10.00\"> (8)</text>\n</g>\n<!-- 137605356745520&#45;&gt;137605363613120 -->\n<g id=\"edge11\" class=\"edge\">\n<title>137605356745520&#45;&gt;137605363613120</title>\n<path fill=\"none\" stroke=\"black\" d=\"M66.61,-495.8C67.32,-486.7 68.25,-474.79 69.02,-464.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"72.52,-465.09 69.81,-454.84 65.54,-464.54 72.52,-465.09\"/>\n</g>\n<!-- 137605363612064 -->\n<g id=\"node14\" class=\"node\">\n<title>137605363612064</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226,-454.5 149,-454.5 149,-435.5 226,-435.5 226,-454.5\"/>\n<text text-anchor=\"middle\" x=\"187.5\" y=\"-442.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 137605363612064&#45;&gt;137605363610816 -->\n<g id=\"edge12\" class=\"edge\">\n<title>137605363612064&#45;&gt;137605363610816</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.5,-435.37C187.5,-426.16 187.5,-411.29 187.5,-399.27\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"191,-398.91 187.5,-388.91 184,-398.91 191,-398.91\"/>\n</g>\n<!-- 137605363612208 -->\n<g id=\"node15\" class=\"node\">\n<title>137605363612208</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-520.5 143,-520.5 143,-501.5 244,-501.5 244,-520.5\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-508.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363612208&#45;&gt;137605363612064 -->\n<g id=\"edge13\" class=\"edge\">\n<title>137605363612208&#45;&gt;137605363612064</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.69,-501.37C191.82,-492.07 190.4,-476.98 189.27,-464.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"192.75,-464.53 188.33,-454.91 185.78,-465.19 192.75,-464.53\"/>\n</g>\n<!-- 137605352767328 -->\n<g id=\"node16\" class=\"node\">\n<title>137605352767328</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"259,-592 128,-592 128,-562 259,-562 259,-592\"/>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\">linearlayer1.weight</text>\n<text text-anchor=\"middle\" x=\"193.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\"> (8, 209)</text>\n</g>\n<!-- 137605352767328&#45;&gt;137605363612208 -->\n<g id=\"edge14\" class=\"edge\">\n<title>137605352767328&#45;&gt;137605363612208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M193.5,-561.8C193.5,-552.7 193.5,-540.79 193.5,-530.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197,-530.84 193.5,-520.84 190,-530.84 197,-530.84\"/>\n</g>\n<!-- 137605363612112 -->\n<g id=\"node17\" class=\"node\">\n<title>137605363612112</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"330,-322.5 253,-322.5 253,-303.5 330,-303.5 330,-322.5\"/>\n<text text-anchor=\"middle\" x=\"291.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 137605363612112&#45;&gt;137605363611200 -->\n<g id=\"edge15\" class=\"edge\">\n<title>137605363612112&#45;&gt;137605363611200</title>\n<path fill=\"none\" stroke=\"black\" d=\"M281.12,-303.37C268.78,-293.12 247.99,-275.84 232.91,-263.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"235.14,-260.61 225.21,-256.91 230.67,-265.99 235.14,-260.61\"/>\n</g>\n<!-- 137605363612400 -->\n<g id=\"node18\" class=\"node\">\n<title>137605363612400</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"358,-388.5 257,-388.5 257,-369.5 358,-369.5 358,-388.5\"/>\n<text text-anchor=\"middle\" x=\"307.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363612400&#45;&gt;137605363612112 -->\n<g id=\"edge16\" class=\"edge\">\n<title>137605363612400&#45;&gt;137605363612112</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.34,-369.37C303.02,-360.07 299.24,-344.98 296.22,-332.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"299.55,-331.76 293.73,-322.91 292.76,-333.46 299.55,-331.76\"/>\n</g>\n<!-- 137605352761088 -->\n<g id=\"node19\" class=\"node\">\n<title>137605352761088</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"375,-460 244,-460 244,-430 375,-430 375,-460\"/>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">linearlayer2.weight</text>\n<text text-anchor=\"middle\" x=\"309.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (8, 8)</text>\n</g>\n<!-- 137605352761088&#45;&gt;137605363612400 -->\n<g id=\"edge17\" class=\"edge\">\n<title>137605352761088&#45;&gt;137605363612400</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.06,-429.8C308.77,-420.7 308.4,-408.79 308.09,-398.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"311.59,-398.73 307.78,-388.84 304.59,-398.95 311.59,-398.73\"/>\n</g>\n<!-- 137605363612016 -->\n<g id=\"node20\" class=\"node\">\n<title>137605363612016</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"391,-196 314,-196 314,-177 391,-177 391,-196\"/>\n<text text-anchor=\"middle\" x=\"352.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 137605363612016&#45;&gt;137605363608656 -->\n<g id=\"edge18\" class=\"edge\">\n<title>137605363612016&#45;&gt;137605363608656</title>\n<path fill=\"none\" stroke=\"black\" d=\"M330.33,-176.98C307.73,-168.3 272.44,-154.75 246.75,-144.88\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"247.67,-141.49 237.08,-141.17 245.16,-148.02 247.67,-141.49\"/>\n</g>\n<!-- 137605363612160 -->\n<g id=\"node21\" class=\"node\">\n<title>137605363612160</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"437,-256.5 336,-256.5 336,-237.5 437,-237.5 437,-256.5\"/>\n<text text-anchor=\"middle\" x=\"386.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 137605363612160&#45;&gt;137605363612016 -->\n<g id=\"edge19\" class=\"edge\">\n<title>137605363612160&#45;&gt;137605363612016</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.48,-237.37C376.56,-228.9 368.91,-215.74 362.71,-205.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.59,-203.05 357.54,-196.17 359.54,-206.57 365.59,-203.05\"/>\n</g>\n<!-- 137605352963376 -->\n<g id=\"node22\" class=\"node\">\n<title>137605352963376</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"479,-328 348,-328 348,-298 479,-298 479,-328\"/>\n<text text-anchor=\"middle\" x=\"413.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">linearlayer3.weight</text>\n<text text-anchor=\"middle\" x=\"413.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (47, 8)</text>\n</g>\n<!-- 137605352963376&#45;&gt;137605363612160 -->\n<g id=\"edge20\" class=\"edge\">\n<title>137605352963376&#45;&gt;137605363612160</title>\n<path fill=\"none\" stroke=\"black\" d=\"M407.51,-297.8C403.59,-288.5 398.43,-276.27 394.2,-266.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"397.34,-264.7 390.23,-256.84 390.89,-267.42 397.34,-264.7\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7d26bed134f0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "y=model(x)\n",
        "make_dot(y.mean(),params=dict(model.named_parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yNXQMgkKTUf"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOW_Yw9S_nCA",
        "outputId": "00b01be9-9c1d-49e7-bb2d-90a304776fc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2207"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#counting the no of parameters it have.\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "IvYV0lsSWY_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO7msJbk_nFN",
        "outputId": "df100e82-fa90-438f-b762-37fbb7a80dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 3.4319\n",
            "Accuracy: 10.40%\n",
            "Precision: 0.0198, Recall: 0.0871, F1-score: 0.0304\n",
            "Epoch [20/1000], Loss: 2.1054\n",
            "Accuracy: 36.91%\n",
            "Precision: 0.2795, Recall: 0.3463, F1-score: 0.2692\n",
            "Epoch [30/1000], Loss: 1.0692\n",
            "Accuracy: 74.50%\n",
            "Precision: 0.6806, Recall: 0.7035, F1-score: 0.6545\n",
            "Epoch [40/1000], Loss: 0.3878\n",
            "Accuracy: 95.97%\n",
            "Precision: 0.9706, Recall: 0.9584, F1-score: 0.9546\n",
            "Epoch [50/1000], Loss: 0.6230\n",
            "Accuracy: 97.32%\n",
            "Precision: 0.9814, Recall: 0.9728, F1-score: 0.9691\n",
            "Epoch [60/1000], Loss: 0.0678\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9814, Recall: 0.9792, F1-score: 0.9784\n",
            "Epoch [70/1000], Loss: 0.5912\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9881, Recall: 0.9787, F1-score: 0.9756\n",
            "Epoch [80/1000], Loss: 0.0808\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [90/1000], Loss: 0.1124\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [100/1000], Loss: 0.0185\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9835, Recall: 0.9833, F1-score: 0.9832\n",
            "Epoch [110/1000], Loss: 0.0030\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [120/1000], Loss: 0.0098\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [130/1000], Loss: 0.0061\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [140/1000], Loss: 0.1343\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9799, Recall: 0.9797, F1-score: 0.9793\n",
            "Epoch [150/1000], Loss: 0.0331\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [160/1000], Loss: 0.0060\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [170/1000], Loss: 0.3412\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [180/1000], Loss: 0.0036\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [190/1000], Loss: 0.0014\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [200/1000], Loss: 0.0005\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [210/1000], Loss: 0.0026\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [220/1000], Loss: 0.0002\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [230/1000], Loss: 0.0021\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [240/1000], Loss: 0.0003\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [250/1000], Loss: 0.0002\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [260/1000], Loss: 0.0003\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [270/1000], Loss: 0.0003\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [280/1000], Loss: 0.4081\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [290/1000], Loss: 0.0003\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9802, Recall: 0.9802, F1-score: 0.9802\n",
            "Epoch [300/1000], Loss: 0.0005\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [310/1000], Loss: 0.0001\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [320/1000], Loss: 0.0225\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [330/1000], Loss: 0.0002\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [340/1000], Loss: 0.0085\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [350/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [360/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [370/1000], Loss: 0.0004\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9799, Recall: 0.9797, F1-score: 0.9793\n",
            "Epoch [380/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [390/1000], Loss: 0.0002\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [400/1000], Loss: 0.0001\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9852, Recall: 0.9843, F1-score: 0.9832\n",
            "Epoch [410/1000], Loss: 0.4669\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [420/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [430/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [440/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [450/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [460/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [470/1000], Loss: 0.0001\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [480/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [490/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [500/1000], Loss: 0.2903\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [510/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [520/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [530/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [540/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [550/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [560/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [570/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [580/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [590/1000], Loss: 0.4488\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [600/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9852, Recall: 0.9843, F1-score: 0.9832\n",
            "Epoch [610/1000], Loss: 0.0012\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [620/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [630/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9844, Recall: 0.9828, F1-score: 0.9819\n",
            "Epoch [640/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [650/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [660/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9852, Recall: 0.9843, F1-score: 0.9832\n",
            "Epoch [670/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [680/1000], Loss: 0.3156\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9797, Recall: 0.9792, F1-score: 0.9770\n",
            "Epoch [690/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [700/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [710/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [720/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9835, Recall: 0.9833, F1-score: 0.9832\n",
            "Epoch [730/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [740/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [750/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [760/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [770/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [780/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9852, Recall: 0.9843, F1-score: 0.9832\n",
            "Epoch [790/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9809, Recall: 0.9807, F1-score: 0.9802\n",
            "Epoch [800/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [810/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [820/1000], Loss: 0.0000\n",
            "Accuracy: 97.32%\n",
            "Precision: 0.9722, Recall: 0.9732, F1-score: 0.9723\n",
            "Epoch [830/1000], Loss: 0.3494\n",
            "Accuracy: 97.32%\n",
            "Precision: 0.9722, Recall: 0.9732, F1-score: 0.9723\n",
            "Epoch [840/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9802, Recall: 0.9802, F1-score: 0.9802\n",
            "Epoch [850/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9766, Recall: 0.9767, F1-score: 0.9765\n",
            "Epoch [860/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [870/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9752, Recall: 0.9762, F1-score: 0.9747\n",
            "Epoch [880/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [890/1000], Loss: 0.2589\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [900/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [910/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [920/1000], Loss: 0.0000\n",
            "Accuracy: 96.98%\n",
            "Precision: 0.9706, Recall: 0.9706, F1-score: 0.9705\n",
            "Epoch [930/1000], Loss: 0.0000\n",
            "Accuracy: 97.65%\n",
            "Precision: 0.9775, Recall: 0.9777, F1-score: 0.9765\n",
            "Epoch [940/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9823, Recall: 0.9813, F1-score: 0.9793\n",
            "Epoch [950/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9802, Recall: 0.9802, F1-score: 0.9802\n",
            "Epoch [960/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9903, Recall: 0.9848, F1-score: 0.9819\n",
            "Epoch [970/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Epoch [980/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9799, Recall: 0.9797, F1-score: 0.9793\n",
            "Epoch [990/1000], Loss: 0.0000\n",
            "Accuracy: 97.99%\n",
            "Precision: 0.9802, Recall: 0.9802, F1-score: 0.9802\n",
            "Epoch [1000/1000], Loss: 0.0000\n",
            "Accuracy: 98.32%\n",
            "Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Final Accuracy: 98.32%\n",
            "Final Precision: 0.9911, Recall: 0.9823, F1-score: 0.9792\n",
            "Final Loss: 0.0000\n",
            "training complete. file saved to DATA.pth\n"
          ]
        }
      ],
      "source": [
        "#the training dataset, we have defined earlier.\n",
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "#suppose if Gpu is available, we can puish our model to the device. otherwise we can have in cpu itself\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Loss is CrossEntropyLoss and optimizer is Adam\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total=0\n",
        "correct=0\n",
        "\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, pred = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        accuracy = correct / total * 100\n",
        "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        print(f'Accuracy: {accuracy:.2f}%')\n",
        "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(f'Final Accuracy: {accuracy:.2f}%')\n",
        "print(f'Final Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n",
        "print(f'Final Loss: {loss.item():.4f}')\n",
        "\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"all_words\": all_the_words,\n",
        "\"tags\": tags\n",
        "}\n",
        "#storing these  in a file, this will serialise it and save it to that file named DATA.pth.\n",
        "FILE = \"DATA.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall metrics\n",
        "accuracy = float(correct) / total * 100\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Final Accuracy: {accuracy:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMFZ8WxwJh17",
        "outputId": "e1de9238-6e3c-48a2-f59e-61dcc625b6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy: 98.32%\n",
            "Precision: 0.99\n",
            "Recall: 0.98\n",
            "F1-Score: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU score calculation\n",
        "def get_reference_response(input_text):\n",
        "    for intent in intents['intents']:\n",
        "        if input_text in intent['patterns']:\n",
        "            return intent['responses'][0]\n",
        "    return \"\"\n",
        "\n",
        "# Example validation set\n",
        "validation_set = [\n",
        "    \"How are you\", \"what is passporting\",\"What is Net Worth\",\n",
        "    \"Are you a chatbot\",\"what do you mean by Bank reserves\", \"what are bank assets\", \"what is cryptocurrency\", \"what is liquidity?\",\"what is liquidity trap?\",\"what is mean by narrow money?\",\n",
        "    \"Tell me your name\",\"Do you know about bail-out in financial markets\", \"Tell me about bailout\", \"What does critics and supporter say about bail-out\",\n",
        "    \"can you recommend me something from here\",\"How much time it will take for delivery\", \"When will this site close?\", \"When are you people open\", \"When will this store be open?\",\n",
        "]\n",
        "\n",
        "# Generate predictions on the validation set\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "for input_text in validation_set:\n",
        "    reference_response = get_reference_response(input_text)\n",
        "    references.append([reference_response])\n",
        "\n",
        "    # Generate response using your model\n",
        "    model_input = BagOfWords(tokenize(input_text), all_the_words)\n",
        "    model_input = torch.tensor(model_input).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    output = model(model_input)\n",
        "    _, predicted_label = output.max(1)\n",
        "    predicted_tag = tags[predicted_label.item()]\n",
        "    predicted_response = [intent['responses'][0] for intent in intents['intents'] if intent['tag'] == predicted_tag][0]\n",
        "    hypotheses.append(predicted_response)\n",
        "\n",
        "# Tokenize references and hypotheses for BLEU calculation\n",
        "references_tokenized = [[tokenize(ref[0])] for ref in references]\n",
        "hypotheses_tokenized = [tokenize(hyp) for hyp in hypotheses]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_scores = [sentence_bleu(ref, hyp) for ref, hyp in zip(references_tokenized, hypotheses_tokenized)]\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f'Final BLEU score: {average_bleu_score:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_KrwxgOnd0O",
        "outputId": "5ec29420-eb60-4df8-bc18-43d1c000ebb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final BLEU score: 0.8947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QfGHwXUAFpE",
        "outputId": "1fe80e92-c2ce-4a5e-b441-18d5b9a07071"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetModel(\n",
              "  (linearlayer1): Linear(in_features=209, out_features=8, bias=True)\n",
              "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linearlayer2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (linearlayer3): Linear(in_features=8, out_features=47, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#loading the data we saved before\n",
        "FILE = \"DATA.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "#defining values\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_the_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "#lets print the model now.\n",
        "model = NeuralNetModel(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxrpZ26WAFwF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def bargain(value):\n",
        "    # tokenize a sentence to tokens\n",
        "    tokens = nltk.word_tokenize(value)\n",
        "    #tagging will help top understand at what category it belong to.\n",
        "    #for eg: walk is a verb phrase,one is a cardinal digit\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    bot_name=\"FinBot\"\n",
        "    # tree representation\n",
        "    entities = nltk.chunk.ne_chunk(tagged)\n",
        "#defining the numbers to the str values\n",
        "    numbers = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7, \"8\": 8, \"9\": 9, \"10\" : 10,\"11\": 11,\"12\": 12,\"13\": 13,\"14\": 14,\"15\": 15,\"16\": 16,\"17\": 17,\"18\": 18,\"19\": 19,\"20\": 20,\"21\": 21,\"22\": 22,\"23\": 23,\"24\": 24,\"25\": 25,\"26\": 26,\"27\": 27,\"28\": 28,\"29\":29,\"30\": 30}\n",
        "    OptionsToReply=[\"Sorry, this is of latest fashion, Can you raise the amount a little bit\",\"This is a very special thing, we can't give you at this much less cost\",\"Oh no sorry. Please raise a little bit\"]\n",
        "    for word, wordType in entities:\n",
        "        word = stemmer.stem(word)\n",
        "#             print (word, wordType)\n",
        "        if (wordType in ['CD'] and word in numbers):\n",
        "               if numbers[word] >20:\n",
        "                 print(\"FinBot:Yes agreed! Now,you can buy the ribbon at this price\")\n",
        "               elif numbers[word] <=20:\n",
        "                 print(f\"{bot_name}: {random.choice(OptionsToReply)}\")\n",
        "               else:\n",
        "                  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAIUWjhoRzXQ",
        "outputId": "bd61c3d9-4ceb-4d48-8fdb-215e1ff4de63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinBot: This is a very special thing, we can't give you at this much less cost\n"
          ]
        }
      ],
      "source": [
        "bargain('15')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYcCoJE1EhuY",
        "outputId": "50105f86-2d3b-406e-8692-def57b5ee527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your Name: Okeke\n",
            "FinBot:Hey, Let's chat! (type 'quit' to exit)Also when you start bargaining give digits\n",
            "Okeke:what is a bank?\n",
            "MoneyBot: A bank in simple terms is a business that makes its profit by paying interest to those who keep money there and charging a higher rate of interest to people/businesses who borrow money from the bank\n",
            "Okeke:what is cryptocurrency?\n",
            "MoneyBot: A cryptocurrency is a digital or virtual currency that uses cryptography and is difficult to counterfeit because of this security feature.\n",
            "Okeke:quit\n"
          ]
        }
      ],
      "source": [
        " bot_name = \"MoneyBot\"\n",
        "#for taking the input\n",
        "name=input(\"Enter Your Name: \")\n",
        "print(\"FinBot:Hey, Let's chat! (type 'quit' to exit)Also when you start bargaining give digits\")\n",
        "while True:\n",
        "  #once the person types his name, then from the next chat onwards the name will be shown\n",
        "    sent=input(name+':')\n",
        "    if sent == \"quit\":\n",
        "        break\n",
        "    #since im going to offer bargaining offer for only one product and its rated price is 30 Rs.\n",
        "    #However the person will tell only numbers ranging from 1 to 30\n",
        "    if sent in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\"]:\n",
        "       bargain(sent)\n",
        "    else:\n",
        "        sent = tokenize(sent)\n",
        "        X = BagOfWords(sent, all_the_words)\n",
        "        X = X.reshape(1, X.shape[0])\n",
        "        X = torch.from_numpy(X).to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "        tag = tags[predicted.item()]\n",
        "\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        prob = probs[0][predicted.item()]\n",
        "        if prob.item() > 0.75:\n",
        "            for intent in intents['intents']:\n",
        "                if tag == intent[\"tag\"]:\n",
        "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "        else:\n",
        "            print(f\"{bot_name}: I do not understand...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRECJTIjEhy9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDXmURF3aB76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd7rMkp3aB_C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwxBPAIUEiEm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOKokynaW1B5oV6GR/rdHgq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}